{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "nav_menu": {
      "height": "252px",
      "width": "333px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DejiangZ/Heart-Rate-Monitoring_PPG/blob/master/%E2%80%9CCS564_exercise_ensemble_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjH98jIfy3ON"
      },
      "source": [
        "# Ensemble: Random Forests and AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HH8NY_1Jy3OO"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzMXPbBxy3OP"
      },
      "source": [
        "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSQ-DBtJy3OP"
      },
      "source": [
        "# To support both python 2 and python 3\n",
        "from __future__ import division, print_function, unicode_literals\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41U5BIrAy3OS"
      },
      "source": [
        "# Bagging ensembles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfTgbynIy3OT"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLtlAYNQZpDA"
      },
      "source": [
        "The following code trains an ensemble of **500** **decision tree** classifiers, each trained on **100** training instances randomly sampled from the training set **with** replacement (`bootstrap=False` for *without* replacement). The `n_jobs` parameter tells Scikit-Learn the number of CPU cores to use for training and predictions (`-1` for all available cores)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XvLR19uy3OV"
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(random_state=42), n_estimators=500,\n",
        "    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97Zv9fmqy3OY"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMddWnOmy3Ob"
      },
      "source": [
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train, y_train)\n",
        "y_pred_tree = tree_clf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred_tree))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIDpYWAZy3Od"
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100)\n",
        "    x2s = np.linspace(axes[2], axes[3], 100)\n",
        "    x1, x2 = np.meshgrid(x1s, x2s)\n",
        "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
        "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
        "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
        "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
        "    if contour:\n",
        "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
        "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
        "    plt.axis(axes)\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn_E8B6By3Og"
      },
      "source": [
        "plt.figure(figsize=(11,4))\n",
        "plt.subplot(121)\n",
        "plot_decision_boundary(tree_clf, X, y)\n",
        "plt.title(\"Decision Tree\", fontsize=14)\n",
        "plt.subplot(122)\n",
        "plot_decision_boundary(bag_clf, X, y)\n",
        "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n70KuPj-y3Oj"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNyOW423y3Oj"
      },
      "source": [
        "bag_clf = BaggingClassifier(\n",
        "    DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=42),\n",
        "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLH_LG6ty3Ol"
      },
      "source": [
        "bag_clf.fit(X_train, y_train)\n",
        "y_pred = bag_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-oXOM8Uy3Op"
      },
      "source": [
        "__max_features__: int, float, string or None, optional (default=\"auto\")<br>\n",
        "The number of features to consider when looking for the best split:\n",
        "* If int, then consider `max_features` features at each split.\n",
        "* If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n",
        "* If \"auto\", then `max_features=sqrt(n_features)`.\n",
        "* If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
        "* If \"log2\", then `max_features=log2(n_features)`.\n",
        "* If None, then `max_features=n_features`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13YMPwH3cmAP"
      },
      "source": [
        "The following code trains a random forest classifier with **500** trees (each limited to maximum **16** nodes), using **all** available CPU cores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-g3RFS9y3Op"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rnd_clf.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqir-4NYy3Os"
      },
      "source": [
        "np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsfsW9audAlF"
      },
      "source": [
        "Scikit-Learn computes the importance of each feature automatically after training and scales the results so that the sum of all importances is equal to 1. You can access the result using `feature_importances_` variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFPL7Vihy3Ou"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
        "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
        "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
        "    print(name, score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6-AajdKy3Ow"
      },
      "source": [
        "rnd_clf.feature_importances_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y97T3Dsoy3Oy"
      },
      "source": [
        "## Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSnbmAncy3Oy"
      },
      "source": [
        "try:\n",
        "    from sklearn.datasets import fetch_openml\n",
        "    mnist = fetch_openml('mnist_784', version=1)\n",
        "    mnist.target = mnist.target.astype(np.int64)\n",
        "except ImportError:\n",
        "    from sklearn.datasets import fetch_mldata\n",
        "    mnist = fetch_mldata('MNIST original')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyooDfuny3O0"
      },
      "source": [
        "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "rnd_clf.fit(mnist[\"data\"], mnist[\"target\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83DTAFkzy3O3"
      },
      "source": [
        "def plot_digit(data):\n",
        "    image = data.reshape(28, 28)\n",
        "    plt.imshow(image, cmap = mpl.cm.hot,\n",
        "               interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdJOJ2rky3O5"
      },
      "source": [
        "plot_digit(rnd_clf.feature_importances_)\n",
        "\n",
        "cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])\n",
        "cbar.ax.set_yticklabels(['Not important', 'Very important'])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMyrEIKAy3O7"
      },
      "source": [
        "# AdaBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImQ1segzdqK0"
      },
      "source": [
        "The following code trains an AdaBoost classifier based on **200** decision stumps (decision trees with `max_depth=1`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5dL1Ivty3O8"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "ada_clf = AdaBoostClassifier(\n",
        "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
        "    algorithm=\"SAMME\", learning_rate=0.5, random_state=42)\n",
        "ada_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7D_ZEbLy3O-"
      },
      "source": [
        "plot_decision_boundary(ada_clf, X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM0MZEcpy3PA"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "m = len(X_train)\n",
        "\n",
        "plt.figure(figsize=(11, 4))\n",
        "for subplot, learning_rate in ((121, 1), (122, 0.5)):\n",
        "    sample_weights = np.ones(m)\n",
        "    plt.subplot(subplot)\n",
        "    for i in range(5):\n",
        "        svm_clf = SVC(kernel=\"rbf\", C=0.05, gamma=\"auto\", random_state=42)\n",
        "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "        y_pred = svm_clf.predict(X_train)\n",
        "        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n",
        "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
        "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
        "    if subplot == 121:\n",
        "        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n",
        "        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n",
        "        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n",
        "        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n",
        "        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ5tEqgey3PD"
      },
      "source": [
        "list(m for m in dir(ada_clf) if not m.startswith(\"_\") and m.endswith(\"_\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "j133m3Ljy3PF"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guNnPMMf8pLb"
      },
      "source": [
        "### Q1: Load the MNIST data and split it into a training set, a validation set, and a test set (50,000:10,000:10,000). Then, train a decision tree, a naive Bayes, and an SVM. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set using *hard* voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifier?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIysdmn2OpO_"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PPiFzBYQYe6"
      },
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=10000, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu5DLT9OQZyD"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld02obwRzAO5"
      },
      "source": [
        "Declare three individual classifiers with propoer hyperparameter setting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ23DasTQfOi"
      },
      "source": [
        "decision_tree_clf = DecisionTreeClassifier(min_samples_leaf=4, random_state=42)\n",
        "naive_bayes_clf = GaussianNB()\n",
        "svm_clf = LinearSVC(random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTm9csOjQib-"
      },
      "source": [
        "estimators = [decision_tree_clf, naive_bayes_clf, svm_clf]\n",
        "for estimator in estimators:\n",
        "    print(\"Training the\", estimator)\n",
        "    estimator.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5ZvI5QsQl33"
      },
      "source": [
        "[estimator.score(X_val, y_val) for estimator in estimators]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU41juBwQqOU"
      },
      "source": [
        "from sklearn.ensemble import VotingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMBp-9b2Qt0S"
      },
      "source": [
        "named_estimators = [\n",
        "    (\"decision_tree_clf\", decision_tree_clf),\n",
        "    (\"naive_bayes_clf\", naive_bayes_clf),\n",
        "    (\"svm_clf\", svm_clf)\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCODGhlWTEsj"
      },
      "source": [
        "voting_clf = VotingClassifier(named_estimators)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkGCFgBgTQgh"
      },
      "source": [
        "voting_clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdDpLMsjQx37"
      },
      "source": [
        "voting_clf.voting = \"hard\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItpUbyVeyDAt"
      },
      "source": [
        "Do you find an ensemble classifier that outperforms all individual classifiers on the validiation set? You can improve the above code using the grid search repeating until a better ensemble is found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUGhCRl6Q5Ws"
      },
      "source": [
        "voting_clf.score(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8G7U2T2xijM"
      },
      "source": [
        "Please show that the accuracy of the ensemble classifier on the test set is higher than that of an individual classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygpNJRjHRANo"
      },
      "source": [
        "voting_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxaL8dv2RDKq"
      },
      "source": [
        "[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}