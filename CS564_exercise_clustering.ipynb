{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DejiangZ/Heart-Rate-Monitoring_PPG/blob/master/CS564_exercise_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA7L1B3QfOOJ"
      },
      "source": [
        "# Clustering: K-Means and DBSCAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcAmcOefOOK"
      },
      "source": [
        "This tutorial is provided by Yu-Chun Lo (howard.lo@nlplab.cc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "nbpresent": {
          "id": "b292081a-becc-4c76-b7db-0643bcc3a7db"
        },
        "id": "JemHSVt0fOOL"
      },
      "source": [
        "# K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAusD-rtTJB"
      },
      "source": [
        "### The Algorithm\n",
        "\n",
        "1. Randomly choose $k$ centroids $C = \\{c_1, c_2, \\dots, c_k\\}$ from the data points $X = \\{x_1, x_2, \\dots, x_n\\} \\in \\mathbb{R}^D $.\n",
        "2. For each data point $x_i$, find the nearest centroid $c_j$ as its corresponding cluster using *sum of squared distance* $ D(x_i, c_j) = \\displaystyle\\sum_{i=1}^{n}{\\| x_i - c_j \\|^2}$.\n",
        "3. For each cluster, update its centroid by computing means value along the dimension of data points in the cluster.\n",
        "4. Compute the displacement between the old and the new centroids and repeat steps 2 and 3 if the displacement is less than a threshold (converged)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "6f4fb01e-c707-4723-8cd2-4008f3ea3584"
        },
        "id": "CoGgF6w7fOOM"
      },
      "source": [
        "# Start from importing necessary packages.\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from IPython.display import display\n",
        "from sklearn import metrics # for evaluations\n",
        "from sklearn.datasets import make_blobs, make_circles # for generating experimental data\n",
        "from sklearn.preprocessing import StandardScaler # for feature scaling\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# make matplotlib plot inline (Only in Ipython).\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "f0e44f6c-b6bf-4977-a44b-bf18c3e801e6"
        },
        "scrolled": true,
        "id": "dq0B5CeRfOOQ"
      },
      "source": [
        "# Generate data.\n",
        "# `random_state` is the seed used by random number generator for reproducibility (default=None).\n",
        "X, y = make_blobs(n_samples=5000,\n",
        "                  n_features=2,\n",
        "                  centers=3,\n",
        "                  random_state=170)\n",
        "\n",
        "# Print data like Ipython's cell output (Only in Ipython, otherwise use `print`).\n",
        "display(X)\n",
        "display(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "16b9a559-9294-4cac-8921-7ac0bc46e34f"
        },
        "id": "EBat-RpJfOOT"
      },
      "source": [
        "# Plot the data distribution (ground truth) using matplotlib `scatter(axis-x, axis-y, color)`.\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "db1a84f5-233c-45c1-b15e-989548255461"
        },
        "scrolled": false,
        "id": "39CjOjkSfOOW"
      },
      "source": [
        "\"\"\" K-means clustering algorithm.\n",
        "\n",
        "Parameters\n",
        "----------\n",
        "n_init: int, optional, default: 10\n",
        "        Number of time the k-means algorithm will be run with different\n",
        "        centroid seeds. The final results will be the best output of\n",
        "        n_init consecutive runs in terms of inertia.\n",
        "\n",
        "init: {'k-means++', 'random', or ndarray, or a callable}, optional\n",
        "        Method for initialization, default to 'k-means++'.\n",
        "\n",
        "        'k-means++': selects initial cluster centers for k-mean\n",
        "        clustering in a smart way to speed up convergence.\n",
        "\n",
        "        'random': generate k centroids from a Gaussian with mean and\n",
        "        variance estimated from the data.\n",
        "\n",
        "tol: float, default: 1e-4\n",
        "        Relative tolerance with regards to inertia to declare convergence\n",
        "        tolerance is computed using `np.mean(np.var(X, axis=0)) * tol)`\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Perform K-means on our data (Train centroids)\n",
        "kmeans = KMeans(n_clusters=3,\n",
        "                n_init=3,\n",
        "                init='random',\n",
        "                tol=1e-4,\n",
        "                random_state=170,\n",
        "                verbose=True).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "676709d6-de25-41ee-a3f0-dfdb5be5b617"
        },
        "id": "S9iIlGupfOOZ"
      },
      "source": [
        "# Retrieve predictions and cluster centers (centroids).\n",
        "display(kmeans.labels_)\n",
        "display(kmeans.cluster_centers_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "f2d84569-10f0-4aca-a6e0-96624785bd88"
        },
        "scrolled": true,
        "id": "lZMFwxKGfOOc"
      },
      "source": [
        "# Plot the predictions.\n",
        "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
        "plt.scatter(kmeans.cluster_centers_[:,0],\n",
        "            kmeans.cluster_centers_[:,1],\n",
        "            c='w', marker='x', linewidths=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "3a82e1e8-0875-4bac-b399-1f5a468c5306"
        },
        "id": "PKoYZwT8fOOf"
      },
      "source": [
        "# We can make new predictions without re-run kmeans (simpily find nearest centroids).\n",
        "X_new = np.array([[10,10], [-10, -10], [-5, 10]])\n",
        "y_pred = kmeans.predict(X_new)\n",
        "\n",
        "\"\"\" The below code is equivalent to:\n",
        "y_pred = KMeans(...).fit_predict(X), but this needs to fit kmeans again.\n",
        "\"\"\"\n",
        "\n",
        "display(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "2be162d7-364f-4e7d-be4a-86018e786a5d"
        },
        "id": "PfvTnXDVfOOi"
      },
      "source": [
        "# We can get distances from data point to every centroid\n",
        "\n",
        "\"\"\" The below code is equivalent to:\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "euclidean_distances(X_new, kmeans.cluster_centers_)\n",
        "\"\"\"\n",
        "\n",
        "kmeans.transform(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "nbpresent": {
          "id": "1a05432e-f7f1-4b57-932c-763e0f2fef49"
        },
        "id": "XRFkbYMnfOOl"
      },
      "source": [
        "### A Smarter Way to Initialize Centroids: K-means++\n",
        "\n",
        "Since *K-means* highly depends on the initialization of the centroids, the clustering results may be converged to a local minimum. We can address this by setting `init='kmeans++'` instead of `'random'`. *K-means++* initializes centroids in a smarter way to speed up convergence. The algorithm is as follows:\n",
        "1. Randomly choose one centroid from the data points.\n",
        "2. For each data point $x_i$, compute the distance $D(x_i, c_j)$ where $c_j$ is nearest to $x_i$.\n",
        "3. Randomly choose one new data point as a new centroid using *weighted probability distribution* proportional to $D(x_i, c_j)^2$.\n",
        "4. Repeat steps 2 and 3 until $k$ centroids have been chosen.\n",
        "5. Now we have initialized centroids, run *K-means* algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "4c72a76e-5dd6-4634-8671-b3f1ea0d8410"
        },
        "id": "d2rkAX99fOOm"
      },
      "source": [
        "# Perform K-means++ on our data.\n",
        "kmeans_plus_plus = KMeans(n_clusters=3,\n",
        "                n_init=3,\n",
        "                init='k-means++',\n",
        "                tol=1e-4,\n",
        "                random_state=170,\n",
        "                verbose=True).fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "8429782e-bcd7-4322-9395-336aef16069d"
        },
        "id": "xk_FCJ0qfOOp"
      },
      "source": [
        "You can see that *K-means++* converges much faster than *K-means*!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "b47424b1-2ba8-4b23-9e5a-d5406d020413"
        },
        "id": "hl3zs1qgfOOp"
      },
      "source": [
        "# Plot the predictions.\n",
        "plt.scatter(X[:,0], X[:,1], c=kmeans_plus_plus.labels_)\n",
        "plt.scatter(kmeans_plus_plus.cluster_centers_[:,0],\n",
        "            kmeans_plus_plus.cluster_centers_[:,1],\n",
        "            c='w', marker='x', linewidths=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "84bf8a8a-edda-41a6-9a14-111a1c6e276e"
        },
        "id": "B5EBOJXPfOOs"
      },
      "source": [
        "## Drawbacks of K-means"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "1f05b8f1-015b-4ebd-9376-45464e73405e"
        },
        "id": "XxC5LNeafOOv"
      },
      "source": [
        "### Drawback 1: Need to choose a right number of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "21d419d7-7866-4a78-8d01-f02bb60b1370"
        },
        "id": "qiZvoMb_fOOw"
      },
      "source": [
        "# Generate data.\n",
        "X, y = make_blobs(n_samples=1000,\n",
        "                  n_features=2,\n",
        "                  centers=3,\n",
        "                  random_state=170)\n",
        "\n",
        "# Plot the data distribution.\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "fc2f3032-81c2-4c1b-8be0-55f4fd8fae68"
        },
        "id": "2ZeUr-wIfOOy"
      },
      "source": [
        "# Run k-means on non-spherical data.\n",
        "y_pred = KMeans(n_clusters=2, random_state=170).fit_predict(X)\n",
        "\n",
        "# Plot the predictions.\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "ef047f7d-4411-43ef-83a2-f1f3c688981e"
        },
        "id": "gvJyn4V_fOO1"
      },
      "source": [
        "### Solution: Measuring cluster quality to determine the number of clusters\n",
        "\n",
        "\n",
        "#### Supervised method\n",
        "*Homogeneity*: Each cluster contains only members of a single class.\n",
        "\n",
        "*Completeness*: All members of a given class are assigned to the same cluster.\n",
        "\n",
        "#### Unsupervised method\n",
        "\n",
        "*Sihouette Coefficient*: Evaluate how well the **compactness** and the **separation** of the clusters are.\n",
        "(Note that the notation below is consistent with the above content.) Using *Sihouette Coefficient*, we can choose an optimal value for number of clusters.\n",
        "\n",
        "***\n",
        "\n",
        "$ a(x_i) $ denotes the **mean intra-cluster distance**. Evaluate the compactness of the cluster to which $x_i$ belongs. (The smaller the more compact)\n",
        "\n",
        "$$ a(x_i) = \\frac{ \\sum_{x_k \\in C_j ,\\ k \\neq i}{D(x_i, x_k)} }{\\left\\vert C_j \\right\\vert - 1} $$  \n",
        "\n",
        "For the data point $x_i$, caculate its average distance to all the other data points in its cluster. (Minusing one in denominator part is to leave out the current data point $x_i$)\n",
        "\n",
        "***\n",
        "\n",
        "$ b(x_i) $ denotes the **mean nearest-cluster distance**. Evaluate how $x_i$ is separated from other clusters. (The larger the more separated)\n",
        "\n",
        "$$ b(x_i) = \\min_{C_j :\\ 1 \\leq j \\leq k ,\\ x_i \\notin C_j} \\left\\{ \\frac{ \\sum_{x_k \\in C_j}{D(x_i, x_k)} }{\\left\\vert C_j \\right\\vert } \\right\\} $$\n",
        "\n",
        "For the data point $x_i$ and all the other clusters not containing $x_i$, caculate its average distance to all the other data points in the given clusters. Find the minimum distance value with respect to the given clusters.\n",
        "\n",
        "***\n",
        "\n",
        "Finally, *Silhouette Coefficient*: $ s(x_i) = \\displaystyle\\frac{b(x_i) - a(x_i)}{\\max\\{a(x_i), b(x_i)\\}},\\ -1 \\leq s(x_i) \\leq 1 $. Want $a(x_i) \\lt b(x_i)$ and $a(x_i) \\to 0$ so as to $s(x_i) \\to 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "3a4feabc-8d61-4b8a-9fea-6f877c8d0b71"
        },
        "id": "HYMYg1YafOO2"
      },
      "source": [
        "# Generate data.\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close together.\n",
        "X, y = make_blobs(n_samples=500,\n",
        "                  n_features=2,\n",
        "                  centers=4,\n",
        "                  cluster_std=1,\n",
        "                  center_box=(-10.0, 10.0),\n",
        "                  shuffle=True,\n",
        "                  random_state=1)\n",
        "\n",
        "# Plot the data distribution.\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "b7dfa8af-03a6-4808-88b8-819d6500cbdd"
        },
        "scrolled": false,
        "id": "Z45BjSiQfOO4"
      },
      "source": [
        "# List of number of clusters\n",
        "range_n_clusters = [2, 3, 4, 5, 6]\n",
        "\n",
        "# For each number of clusters, perform Silhouette analysis and visualize the results.\n",
        "for n_clusters in range_n_clusters:\n",
        "\n",
        "    # Perform k-means.\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "    # Compute the cluster homogeneity and completeness.\n",
        "    homogeneity = metrics.homogeneity_score(y, y_pred)\n",
        "    completeness = metrics.completeness_score(y, y_pred)\n",
        "\n",
        "    # Compute the Silhouette Coefficient for each sample.\n",
        "    s = metrics.silhouette_samples(X, y_pred)\n",
        "\n",
        "    # Compute the mean Silhouette Coefficient of all data points.\n",
        "    s_mean = metrics.silhouette_score(X, y_pred)\n",
        "\n",
        "    # For plot configuration -----------------------------------------------------------------------------------\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # Configure plot.\n",
        "    plt.suptitle('Silhouette analysis for K-Means clustering with n_clusters: {}'.format(n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Configure 1st subplot.\n",
        "    ax1.set_title('Silhouette Coefficient for each sample')\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "    ax1.set_xlim([-1, 1])\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Configure 2st subplot.\n",
        "    ax2.set_title('Homogeneity: {}, Completeness: {}, Mean Silhouette score: {}'.format(homogeneity,\n",
        "                                                                                        completeness,\n",
        "                                                                                        s_mean))\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    # For 1st subplot ------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Plot Silhouette Coefficient for each sample\n",
        "    cmap = cm.get_cmap(\"Spectral\")\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        ith_s = s[y_pred == i]\n",
        "        ith_s.sort()\n",
        "        size_cluster_i = ith_s.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        color = cmap(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_s,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "        y_lower = y_upper + 10\n",
        "\n",
        "    # Plot the mean Silhouette Coefficient using red vertical dash line.\n",
        "    ax1.axvline(x=s_mean, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    # For 2st subplot -------------------------------------------------------------------------------------------\n",
        "\n",
        "    # Plot the predictions\n",
        "    colors = cmap(y_pred.astype(float) / n_clusters)\n",
        "    ax2.scatter(X[:,0], X[:,1], c=colors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "ab8a33f3-4dee-46e8-9abd-7d2fc56e63d8"
        },
        "id": "bLJcNefvfOO7"
      },
      "source": [
        "The silhouette plot shows that the `n_clusters` value of 3, 5 and 6 are a bad pick for the given data due to the presence of clusters with above average silhouette scores and also due to wide fluctuations in the size of the silhouette plots. Silhouette analysis is more ambivalent in deciding between 2 and 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "ec546f74-36ca-4785-9ff5-1091a032b66c"
        },
        "id": "G0NyYnL1fOO7"
      },
      "source": [
        "### Drawback 2: Cannot handle noise data and outliers\n",
        "\n",
        "Even noise data and outliers are easily observed from the following clustering results (the data points which are relatively far away from the centroids), *K-means* still puts them into the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "37969ee2-e0ac-4dd9-8a02-13b393d5328b"
        },
        "id": "YKGEesBAfOO8"
      },
      "source": [
        "# Generate data.\n",
        "# This particular setting has one distinct cluster and 3 clusters placed close together.\n",
        "# (Same as the above example)\n",
        "X, y = make_blobs(n_samples=500,\n",
        "                  n_features=2,\n",
        "                  centers=4,\n",
        "                  cluster_std=1,\n",
        "                  center_box=(-10.0, 10.0),\n",
        "                  shuffle=True,\n",
        "                  random_state=1)\n",
        "\n",
        "# Perform k-means with n_clusters=4\n",
        "kmeans = KMeans(n_clusters=4, random_state=10)\n",
        "y_pred = kmeans.fit_predict(X)\n",
        "\n",
        "# Plot the prediction\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "a9eb7a4c-3079-4ad5-8d2e-9ff9d510c0b6"
        },
        "id": "W98VnKpNfOO_"
      },
      "source": [
        "### Solution: Use a distance threshold to detect noise data and outliers\n",
        "\n",
        "However, we can detect the noises/outliers conditioning on whether the distance between the data point $x_i$ and the centroid $c_j$ of $x_i$'s corresponding cluster is larger than the average distance in the cluster. That is to say:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "  x_i=\\left\\{\n",
        "  \\begin{array}{@{}ll@{}}\n",
        "    \\text{Outlier}, & \\text{if}\\ D(x_i, c_j) \\gt \\frac{1}{\\left\\vert Cluster_j \\right\\vert} \\sum_{k=0,\\ k \\neq i}^{\\left\\vert Cluster_j \\right\\vert}{D(x_k,c_j)} \\\\\n",
        "    \\text{Non-outlier}, & \\text{otherwise}\n",
        "  \\end{array}\\right.\n",
        "  \\text{where } c_j \\in Cluster_j\n",
        "\\end{equation}\n",
        "$$\n",
        "\n",
        "Let's begin to find out the outliers of each cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "20ed43b9-96c5-4fd7-964b-a0a760556de3"
        },
        "id": "GbOirykvfOPA"
      },
      "source": [
        "# Ratio for our distance threshold, controlling how many outliers we want to detect.\n",
        "distance_threshold_ratio = 2.0\n",
        "\n",
        "# Plot the prediction same as the above.\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)\n",
        "\n",
        "# For each ith cluster, i=0~3 (we have 4 clusters in this example).\n",
        "for i in [0, 1, 2, 3]:\n",
        "\n",
        "    # Retrieve the indexs of data points belong to the ith cluster.\n",
        "    # Note: `np.where()` wraps indexs in a tuple, thus we retrieve indexs using `tuple[0]`\n",
        "    indexs_of_X_in_ith_cluster = np.where(y_pred == i)[0]\n",
        "\n",
        "    # Retrieve the data points by the indexs\n",
        "    X_in_ith_cluster = X[indexs_of_X_in_ith_cluster]\n",
        "\n",
        "    # Retrieve the centroid.\n",
        "    centroid = kmeans.cluster_centers_[i]\n",
        "\n",
        "    # Compute distances between data points and the centroid.\n",
        "    # Same as: np.sqrt(np.sum(np.square(X_in_ith_cluster - centroid), axis=1))\n",
        "    # Note: distances.shape = (X_in_ith_cluster.shape[0], 1). A 2-D matrix.\n",
        "    centroid.shape = (1,-1)\n",
        "    distances = metrics.pairwise.euclidean_distances(X_in_ith_cluster, centroid)\n",
        "\n",
        "    # Compute the mean distance for ith cluster as our distance threshold.\n",
        "    distance_threshold = np.mean(distances)\n",
        "\n",
        "    # Retrieve the indexs of outliers in ith cluster\n",
        "    # Note: distances.flatten() flattens 2-D matrix to vector, in order to compare with scalar `distance_threshold`.\n",
        "    indexs_of_outlier = np.where(distances.flatten() > distance_threshold * distance_threshold_ratio)[0]\n",
        "\n",
        "    # Retrieve outliers in ith cluster by the indexs\n",
        "    outliers = X_in_ith_cluster[indexs_of_outlier]\n",
        "\n",
        "    # Plot the outliers in ith cluster.\n",
        "    plt.scatter(outliers[:,0], outliers[:,1], c='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "a2bbd483-bb55-48f0-bc5a-16dc4d5a4fa3"
        },
        "id": "TFrwjTTIfOPC"
      },
      "source": [
        "As we've mentioned about measuring cluster quality analysis, you can run different settings of `distance_threshold_ratio` to find out the best cluster quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "478a0112-e819-41ad-84dd-3f4be5bccf60"
        },
        "id": "X5DILY9KfOPD"
      },
      "source": [
        "### Drawback 3: Cannot handle non-spherical data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZnHtFv1fOPD"
      },
      "source": [
        "> *K-means* clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with **the nearest mean**. (Wikipedia)\n",
        "\n",
        "Since the concentric circles would have the approximately same mean, so k-means is not suitable to separate them.\n",
        "\n",
        "Let's generate non-spherical data and plot the ground truths of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "b06a8291-663b-4d92-89ca-033db1e0d247"
        },
        "id": "rB7R2Ng9fOPE"
      },
      "source": [
        "# Generate non-spherical data.\n",
        "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n",
        "\n",
        "# Plot the data distribution. (Here's another way to plot scatter graph)\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], 'ro')\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], 'go')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqGVDzqAfOPG"
      },
      "source": [
        "After performing *K-means* on non-spherical data, the following result shows that it fails to cluster non-spherical data, since *K-means* has an assumption that the data distribution is spherical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "c6d61883-e8b2-4a96-b93d-17f9e4e980b6"
        },
        "id": "o6pFwhJ0fOPH"
      },
      "source": [
        "# Run k-means on non-spherical data.\n",
        "y_pred = KMeans(n_clusters=2, random_state=170).fit_predict(X)\n",
        "\n",
        "# Plot the predictions.\n",
        "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
        "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')\n",
        "\n",
        "# Print the evaluations\n",
        "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
        "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
        "print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "5572f1cd-9361-4ffb-aac4-b75850a6c6bd"
        },
        "id": "WAncBjz-fOPJ"
      },
      "source": [
        "### Solution: Using feature transformation or extraction techiques makes data clusterable\n",
        "\n",
        "If you know that your clusters will always be concentric circles, you can simply convert your cartesian (x-y) coordinates to polar coordinates, and use only the radius for clustering - as you know that the angle theta doesn't matter.\n",
        "\n",
        "Or more generally: use a suitable kernel for k-means clustering, e.g. use *Kernel PCA* to find a projection of the data that makes data linearly separable, or use another clustering algorithm, such as *DBSCAN*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMikmzUjfOPJ"
      },
      "source": [
        "def cart2pol(x, y):\n",
        "    radius = np.sqrt(x**2 + y**2)\n",
        "    theta = np.arctan2(y, x)\n",
        "    return radius, theta\n",
        "\n",
        "X_transformed = np.zeros_like(X)\n",
        "X_transformed[:,0], X_transformed[:,1] = cart2pol(X[:,0], X[:,1])\n",
        "\n",
        "plt.plot(X_transformed[y == 0, 0], X_transformed[y == 0, 1], 'ro')\n",
        "plt.plot(X_transformed[y == 1, 0], X_transformed[y == 1, 1], 'go')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bx040a7fOPM"
      },
      "source": [
        "We just successfully make data linearly separable by converting features (x-y) to (radius-theta) !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5uwYRsufOPM"
      },
      "source": [
        "def cart2pol(x, y):\n",
        "    radius = np.sqrt(x**2 + y**2)\n",
        "    theta = np.arctan2(y, x)\n",
        "    return radius, theta\n",
        "\n",
        "X_transformed = np.zeros_like(X)\n",
        "# Convert cartesian (x-y) to polar coordinates.\n",
        "X_transformed[:,0], _ = cart2pol(X[:,0], X[:,1])\n",
        "\n",
        "# Only use `radius` feature to cluster.\n",
        "y_pred = KMeans(n_clusters=2).fit_predict(X_transformed)\n",
        "\n",
        "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
        "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "f2622e52-6690-4b5a-b604-a3eb7d12a2aa"
        },
        "id": "Iqooa1BifOPO"
      },
      "source": [
        "Now the data is successfully clustered!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrZDJKiato7L"
      },
      "source": [
        "### Drawback 4: Cannot to handle categorical data\n",
        "Since *k-means* only works when *mean* is defined, categorical data cannot compute mean value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DGj1byrtphv"
      },
      "source": [
        "### Solution: Please refer to [kmodes](https://github.com/nicodv/kmodes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "9adbb49f-133f-497d-952c-256a473da64e"
        },
        "id": "x_2ORrKBfOPP"
      },
      "source": [
        "# DBSCAN: Density-Based Spatial Clustering of Applications with Noise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtIVNMdouP13"
      },
      "source": [
        "### Parameters\n",
        "\n",
        "- $Eps$: Maximum radius of the neighborhood.\n",
        "- $MinPts$: Minimum number of points in the Eps-neighborhood of a point."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18tKOVsjuRkz"
      },
      "source": [
        "### Terms\n",
        "\n",
        "- The Eps-neighborhood of a point $q$Ôºç$N_{Eps}$: A point $p \\in N_{Eps}(q)$ if $D(p,q) \\leq Eps$. (Point inside the circle).\n",
        "- Outlier: Not in a cluster.\n",
        "- Core point: $\\left\\vert N_{Eps}(q) \\right\\vert \\geq MinPts$ (dense neighborhood).\n",
        "- Border point: In cluster but neighborhood is not dense.\n",
        "\n",
        "<div style=\"text-align:center\"><img width=\"300px\" src=\"https://drive.google.com/uc?id=1C0cGNRMRi_rMqFJt4BixTHop5yhM_qB8\"/></div>\n",
        "\n",
        "- Directly density-reachable: A point $p$ is **directly density-reachable** from a point $q$ w.r.t $Eps$ and $MinPts$ if:\n",
        "    - $p \\in N_{Eps}(q)$, and $q$ is a **core point**.\n",
        "    - $p$ **doesn't** need to be a core point.\n",
        "\n",
        "<div style=\"text-align:center\"><img width=\"250px\" src=\"https://drive.google.com/uc?id=1WpTc8-sdNukzbi2FxMQJau6B-j1afjXx\"/></div>\n",
        "\n",
        "- Density-reachable: A point $p$ is **density-reachable** from a point $q$ w.r.t. $Eps$ and $MinPts$ if there is a chain of points $p_1, \\dots, p_n,\\ p_1 = q,\\ p_n = p$ such that $p_{i+1}$ is directly density-reachable from $p_i$.\n",
        "\n",
        "<div style=\"text-align:center\"><img width=\"150px\" src=\"https://drive.google.com/uc?id=1itFwxp5kvjiytX_KFQRRfuoJXHpsku0d\"/></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cA8JwJZuUY2"
      },
      "source": [
        "### The Algorithm\n",
        "1. Randomly choose a point $p$.\n",
        "2. Retrieve all points density-reachable from $p$ w.r.t. $Eps$ and $MinPts$.\n",
        "3. If $p$ is a core point, a cluster is formed.\n",
        "4. If $p$ is a border point, no points are density-reachable from $p$, then visit the next point.\n",
        "5. Repeat the process until all the data points have been processed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi34QmvdfOPP"
      },
      "source": [
        "Let's begin to perform *DBSCAN* on spherical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "f3676519-cac4-4445-b29a-fb61e9ac8882"
        },
        "id": "v021BjVAfOPQ"
      },
      "source": [
        "# Generate data with 3 centers.\n",
        "X, y = make_blobs(n_samples=1000,\n",
        "                  n_features=2,\n",
        "                  centers=3,\n",
        "                  random_state=170)\n",
        "\n",
        "# Standardize features to zero mean and unit variance.\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Perform DBSCAN on the data\n",
        "y_pred = DBSCAN(eps=0.3, min_samples=30).fit_predict(X)\n",
        "\n",
        "# Plot the predictions\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)\n",
        "\n",
        "# Print the evaluations\n",
        "print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
        "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
        "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
        "print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpH2P-KxfOPT"
      },
      "source": [
        "The **black** data points denote the **outliers** in the above result.\n",
        "\n",
        "Note that we don't need to specify the number of clusters with *DBSCAN* algorithm. Besides, *DBSCAN* is good at finding out the outliers without requiring some hacks like we did above in *K-means* section.\n",
        "\n",
        "Now, let's try *DBSCAN* on non-spherical data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "nbpresent": {
          "id": "4795d38a-ea38-49e7-9e3b-b9812c80f03b"
        },
        "id": "E1nBZwmpfOPU"
      },
      "source": [
        "# Generate non-spherical data.\n",
        "X, y = make_circles(n_samples=1000, factor=0.3, noise=0.1)\n",
        "\n",
        "# Standardize features to zero mean and unit variance.\n",
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Perform DBSCAN on the data\n",
        "y_pred = DBSCAN(eps=0.3, min_samples=10).fit_predict(X)\n",
        "\n",
        "# Plot the data distribution. (Here's another way to plot scatter graph)\n",
        "plt.plot(X[y_pred == 0, 0], X[y_pred == 0, 1], 'ro')\n",
        "plt.plot(X[y_pred == 1, 0], X[y_pred == 1, 1], 'go')\n",
        "\n",
        "# Print the evaluations\n",
        "print('Number of clusters: {}'.format(len(set(y_pred[np.where(y_pred != -1)]))))\n",
        "print('Homogeneity: {}'.format(metrics.homogeneity_score(y, y_pred)))\n",
        "print('Completeness: {}'.format(metrics.completeness_score(y, y_pred)))\n",
        "print('Mean Silhouette score: {}'.format(metrics.silhouette_score(X, y_pred)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "c95f7334-c3cb-467b-8bd0-f948a234a75e"
        },
        "id": "3VyIkIZJfOPW"
      },
      "source": [
        "Comparing to *K-means*, we can directly apply *DBSCAN* on this form of data distribution due to the density-based clustering criterion.\n",
        "\n",
        "Note: It's worth mention that the *Silhouette score* is generally higher for **convex** clusters than other concepts of clusters, such as density based clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISgZx5oKrbGn"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgcsQZTTriK1"
      },
      "source": [
        " ### Q1: Please generate your own 2-D training data and visualize the ground truth.<br>\n",
        " Notes on `make_blobs()`:\n",
        " - `cluster_std`: The standard deviation of the clusters (the degree of data dispersion in clusters), e.g., a floating number.\n",
        " - `center_box`: The bounding box for each cluster center when centers are generated at random, e.g., a tuple `(min, max)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqxwk1dwrwWn"
      },
      "source": [
        "X, y = make_blobs(n_samples=1000,\n",
        "                  n_features=2,\n",
        "                  centers=3,\n",
        "                  cluster_std=0.5,\n",
        "                  center_box=(-5,5))\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJGzSUMcrqSE"
      },
      "source": [
        "### Q2: Apply *k-means* on your training data and visualize the clusters and centroids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTKgJ4PFsG1x"
      },
      "source": [
        "kmeans = KMeans(n_clusters=3,\n",
        "                n_init=10,\n",
        "                tol=1e-4,\n",
        "                verbose=True).fit(X)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=kmeans.labels_)\n",
        "plt.scatter(kmeans.cluster_centers_[:,0],\n",
        "            kmeans.cluster_centers_[:,1],\n",
        "            c='w', marker='x', linewidths=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmSLMYysnYn3"
      },
      "source": [
        "### Q3: Apply *DBSCAN* on the following data set with the proper parameter values. Then, visualize the result and discuss its validity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsDwMymloMvP"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "X, y = make_blobs(n_samples=1500, cluster_std=[1.0, 2.5, 0.5], random_state=170)\n",
        "plt.scatter(X[:,0], X[:,1], c=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U36L2Jh0qrcV"
      },
      "source": [
        "X = StandardScaler().fit_transform(X)\n",
        "\n",
        "y_pred = DBSCAN(eps=0.31, min_samples=27).fit_predict(X)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}